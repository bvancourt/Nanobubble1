"""
This module provides basic functionality for working with images from the VEVO 
2100 Ultrasound machine in python as well as functionality for pre-processing
and quality control.
"""

import os
import warnings
import copy
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

class ClipPacket:
    """
    ClipPacket objects should basically contain all the important information 
    from the files generated by exporting a cine-loop from VEVO Lab in RAW 
    format. Made specifically for contrast mode cine-loops from the VEVO 2100, 
    but writen to hopefully be close to compatible with other sets of VEVO RAW 
    files.
    """

    def __init__(
        self,
        meta_data,
        frame_times,
        bmode_pixels,
        contrast_pixels,
        physio_times,
        breath,
        ekg,
        source_base_path=None,
        time_offset=0,
    ):
        self.meta_data = meta_data
        self.frame_times = frame_times
        self.bmode_pixels = bmode_pixels
        self.contrast_pixels = contrast_pixels
        self.physio_times = physio_times
        self.breath = breath
        self.ekg = ekg
        self.time_offset = time_offset
        self.source_base_path = source_base_path

    def calculate_bmode_offness_signal(self):
        mean_image = np.mean(self.bmode_pixels.astype(np.float32), axis=0)
        offness_signal = np.zeros(self.bmode_pixels.shape[0])
        for i_frame in range(self.bmode_pixels.shape[0]):
            frame_difference_from_mean = (
                self.bmode_pixels[i_frame, :, :].astype(np.float32) - mean_image
            )
            offness_signal[i_frame] = np.sum(
                frame_difference_from_mean * frame_difference_from_mean
            )
        self._bmode_offness_signal = (
            offness_signal - np.mean(offness_signal)
        ) / np.std(offness_signal)

    @property
    def bmode_offness_signal(self):
        if hasattr(self, "_bmode_offness_signal"):
            return self._bmode_offness_signal
        else:
            self.calculate_bmode_offness_signal()
            return self._bmode_offness_signal

    @property
    def time_since_ekg_spike(self):
        z_score_ekg = (self.ekg - np.mean(self.ekg)) / np.maximum(
            100, np.std(self.ekg)
        )
        beat_events = (z_score_ekg > 2) & ~np.roll((z_score_ekg > 2), 1)

        event_indices = np.arange(len(self.ekg))[beat_events]
        time_since_event = np.full(len(self.ekg), np.nan)
        for first_i, second_i in zip(event_indices[:-1], event_indices[1:]):
            time_since_event[first_i:second_i] = np.arange(second_i - first_i)

        return time_since_event / self.meta_data["Physio-Clock"]

    @property
    def ekg_phase(self):
        z_score_ekg = (self.ekg - np.mean(self.ekg)) / np.maximum(
            100, np.std(self.ekg)
        )
        beat_events = (z_score_ekg > 2) & ~np.roll((z_score_ekg > 2), 1)

        event_indices = np.arange(len(self.ekg))[beat_events]
        ekg_phase = np.full(len(self.ekg), np.nan)
        for i_event, (first_i, second_i) in enumerate(
            zip(event_indices[:-1], event_indices[1:])
        ):
            ekg_phase[first_i:second_i] = i_event + np.arange(
                second_i - first_i
            ) / (second_i - first_i)

        return ekg_phase

    def image_physio_sync_plot(self, show=True, save_path=None):
        fig, ax1 = plt.subplots(figsize=[10, 6])
        ax1.set_title("Image/Physio Time Synchronization Check")
        ax1.plot(self.frame_times, self.bmode_offness_signal, color="blue")
        ax1.set_xlabel("Time (s)")
        ax1.set_ylabel('B-Mode Frame "Offness"', color="blue")

        ax2 = ax1.twinx()
        ax2.plot(
            self.physio_times,
            (self.breath - np.mean(self.breath)) / np.std(self.breath),
            color="green",
        )
        ax2.set_ylabel("Breath Signal Z-score", color="green")

        if not save_path == None:
            plt.savefig(
                save_path, bbox_inches="tight", transparent=True, dpi=600
            )

        if show:
            plt.show()

    def one_panel_contrast_overlay(
        self,
        frame="mean",
        show=True,
        save_path=None,
        no_axes=False,
        dpi=600,
        fontsize=12,
    ):
        if self.meta_data["full_set"]:
            if frame == "mean":
                bmode_disp_pixels = np.mean(
                    self.bmode_pixels.astype(np.float32), axis=0
                )
                contrast_disp_pixels = np.mean(
                    self.contrast_pixels.astype(np.float32), axis=0
                )
            elif type(frame) == int:
                if (frame >= 0) and (frame < self.bmode_pixels.shape[0]):
                    bmode_disp_pixels = self.bmode_pixels[frame, :, :].astype(
                        np.float32
                    )
                    contrast_disp_pixels = self.contrast_pixels[
                        frame, :, :
                    ].astype(np.float32)
                else:
                    warnings.warn(
                        f"frame number {frame} out of range for "
                        + f"bmode_pixels.shape[0]={self.bmode_pixels.shape[0]}."
                    )
                    return
            else:
                warnings.warn(
                    'Kwarg "frame" must be either "mean" or an integer '
                    + "specifying which frame to display."
                )
                return

            # add images
            im1 = plt.imshow(
                bmode_disp_pixels,
                extent=self.meta_data["bmode_extent"],
                cmap="gray",
            )
            im2 = plt.imshow(
                np.log(np.clip(contrast_disp_pixels, 0.5, 2**16)),
                extent=self.meta_data["contrast_extent"],
                cmap="inferno",
                alpha=1,
            )

            # add frame around contrast region
            plt.plot(
                [
                    np.min(self.meta_data["contrast_x_mm"]),
                    np.min(self.meta_data["contrast_x_mm"]),
                ],
                [
                    np.min(self.meta_data["contrast_y_mm"]),
                    np.max(self.meta_data["contrast_y_mm"]),
                ],
                color="green",
                linewidth=fontsize / 4,
            )
            plt.plot(
                [
                    np.max(self.meta_data["contrast_x_mm"]),
                    np.max(self.meta_data["contrast_x_mm"]),
                ],
                [
                    np.min(self.meta_data["contrast_y_mm"]),
                    np.max(self.meta_data["contrast_y_mm"]),
                ],
                color="green",
                linewidth=fontsize / 4,
            )
            plt.plot(
                [
                    np.min(self.meta_data["contrast_x_mm"]),
                    np.max(self.meta_data["contrast_x_mm"]),
                ],
                [
                    np.min(self.meta_data["contrast_y_mm"]),
                    np.min(self.meta_data["contrast_y_mm"]),
                ],
                color="green",
                linewidth=fontsize / 4,
            )
            plt.plot(
                [
                    np.min(self.meta_data["contrast_x_mm"]),
                    np.max(self.meta_data["contrast_x_mm"]),
                ],
                [
                    np.max(self.meta_data["contrast_y_mm"]),
                    np.max(self.meta_data["contrast_y_mm"]),
                ],
                color="green",
                linewidth=fontsize / 4,
            )

            if no_axes:
                plt.gca().set_axis_off()

            plt.gca().tick_params(
                axis="both", which="major", labelsize=fontsize * 5 / 6
            )
            plt.ylabel("Depth (mm)", fontsize=fontsize)
            plt.xlabel("Line Position (mm)", fontsize=fontsize)

            if not save_path == None:
                plt.savefig(
                    save_path, bbox_inches="tight", transparent=True, dpi=dpi
                )

            if show:
                plt.show()

        else:
            warnings.warn(
                'one_panel_contrast_overlay() requires a "full_set" ClipPacket.'
            )
            return

    @classmethod
    def from_file_paths_dict(
        cls,
        file_paths_dict,
        mouse_ID="unknown",
        clip_index=-1,
        time_offset=0,
        snap_contrast_to_bmode=True,
        source_base_path=None,
    ):

        if "physio" in file_paths_dict:
            breath, ekg, physio_frame_headers, _ = read_physio_raw(
                file_paths_dict["physio"]
            )
            physio_times = (
                np.arange(len(breath)) / 8000
            )  # this will be recalculated if there is an xml file.
        else:
            breath, ekg, physio_frame_headers, physio_times = (
                None,
                None,
                None,
                None,
            )

        if "bmode" in file_paths_dict:
            bmode_pixels, bmode_frame_headers, _ = read_bmode_pixels(
                file_paths_dict["bmode"], return_headers=True
            )
            frame_times = (
                bmode_frame_headers[:, 0] / 400000
            )  # this will be recalculated if there is an xml file.
        else:
            bmode_pixels, bmode_frame_headers, frame_times = None, None, None

        if "xml" in file_paths_dict:
            xml_dict = read_raw_xml(file_paths_dict["xml"])

            if "contrast" in file_paths_dict:
                contrast_pixels = read_contrast_pixels(
                    file_paths_dict["contrast"],
                    [
                        xml_dict["Nonlinear-Contrast-Mode/Samples"],
                        xml_dict["Nonlinear-Contrast-Mode/Lines"],
                    ],
                )

            else:
                contrast_pixels = None

            if "bmode" in file_paths_dict:
                frame_times = (
                    bmode_frame_headers[:, 0] / xml_dict["Time-Stamp-Clock"]
                )

            if "physio" in file_paths_dict:
                breath, ekg, physio_frame_headers, _ = read_physio_raw(
                    file_paths_dict["physio"]
                )
                physio_times = np.arange(len(breath)) / xml_dict["Physio-Clock"]

        else:
            xml_dict = {}

        meta_data = copy.deepcopy(xml_dict)
        meta_data.update(
            {
                "mouse_ID": mouse_ID,
                "clip_index": clip_index,
                "time_offset": time_offset,
                "full_set": False,
                "file_paths": file_paths_dict,
            }
        )

        # Note: It might be possible to not require the physio file below.
        full_set = (
            ("physio" in file_paths_dict)
            and ("bmode" in file_paths_dict)
            and ("xml" in file_paths_dict)
            and ("contrast" in file_paths_dict)
        )
        if full_set:  # if all files are present, perform space synchronization
            meta_data["full_set"] = True

            meta_data["bmode_pixel_height"] = (
                xml_dict["B-Mode/Depth"] - xml_dict["B-Mode/Depth-Offset"]
            ) / (bmode_pixels.shape[1])
            meta_data["bmode_pixel_width"] = (xml_dict["B-Mode/Width"]) / (
                bmode_pixels.shape[2]
            )
            meta_data["bmode_y_mm"] = (
                np.arange(bmode_pixels.shape[1])
                * meta_data["bmode_pixel_height"]
                + xml_dict["B-Mode/Depth-Offset"]
            )

            if (
                bmode_pixels.shape[1] % 2 == 0
            ):  # To the best of my knowledge this is always true.
                meta_data["bmode_x_mm"] = (
                    np.arange(
                        -bmode_pixels.shape[2] // 2, bmode_pixels.shape[2] // 2
                    )
                    + 0.5
                ) * meta_data["bmode_pixel_width"] + xml_dict["B-Mode/Centre"]
            else:
                meta_data["bmode_x_mm"] = (
                    (
                        np.arange(
                            -bmode_pixels.shape[2] // 2,
                            bmode_pixels.shape[2] // 2,
                        )
                        + 1
                    )
                    * meta_data["bmode_pixel_width"]
                    + xml_dict["B-Mode/Centre"]
                    + 0.5
                )

            contrast_center_x = xml_dict["Nonlinear-Contrast-Mode/Centre"]
            contrast_center_y = (
                xml_dict["Nonlinear-Contrast-Mode/Depth"]
                + xml_dict["Nonlinear-Contrast-Mode/Depth-Offset"]
            ) / 2
            contrast_top_y = (
                contrast_center_y
                - meta_data["bmode_pixel_height"] * contrast_pixels.shape[1] / 2
            )
            contrast_left_x = (
                contrast_center_x
                - meta_data["bmode_pixel_width"] * contrast_pixels.shape[2] / 4
            )

            if snap_contrast_to_bmode:
                contrast_top_y = meta_data["bmode_y_mm"][
                    np.argmin(np.abs(meta_data["bmode_y_mm"] - contrast_top_y))
                ]

            # Assuming that contrast pixels are exactly the same height and 
            #   half the width of bmode pixels.
            meta_data["contrast_y_mm"] = (
                np.arange(contrast_pixels.shape[1])
                * meta_data["bmode_pixel_height"]
                + contrast_top_y
            )
            meta_data["contrast_x_mm"] = (
                np.arange(contrast_pixels.shape[2])
                * meta_data["bmode_pixel_width"]
                / 2
                + contrast_left_x
            )

            meta_data["contrast_extent"] = (
                np.min(meta_data["contrast_x_mm"]),
                np.max(meta_data["contrast_x_mm"]),
                np.max(meta_data["contrast_y_mm"]),
                np.min(meta_data["contrast_y_mm"]),
            )
            meta_data["bmode_extent"] = (
                np.min(meta_data["bmode_x_mm"]),
                np.max(meta_data["bmode_x_mm"]),
                np.max(meta_data["bmode_y_mm"]),
                np.min(meta_data["bmode_y_mm"]),
            )

        else:
            meta_data["full_set"] = False

        return cls(
            meta_data,
            frame_times,
            bmode_pixels,
            contrast_pixels,
            physio_times,
            breath,
            ekg,
            source_base_path=source_base_path,
        )

    @classmethod
    def from_base_path(cls, base_path, mouse_ID="unknown", clip_index=-1):
        # Construct ClipPacket from common base path to files.
        # File names must end in ".raw.[bmode, physio, xml, etc.]"

        if not base_path[-4:] == ".raw":
            # This is just so that ".raw" can be included in the base path 
            #    or not.
            base_path = base_path + ".raw"

        dict_of_RAW_file_paths = {}

        # More paths would have to be added below if there were other files.
        bmode_path = base_path + ".bmode"
        contrast_path = base_path + ".contrast"
        physio_path = base_path + ".physio"
        # event_path = base_path + '.event' # This may be unused.
        xml_path = base_path + ".xml"

        if os.path.exists(xml_path):
            dict_of_RAW_file_paths["xml"] = xml_path
        if os.path.exists(physio_path):
            dict_of_RAW_file_paths["physio"] = physio_path
        if os.path.exists(bmode_path):
            dict_of_RAW_file_paths["bmode"] = bmode_path
        if os.path.exists(contrast_path):
            dict_of_RAW_file_paths["contrast"] = contrast_path

        return cls.from_file_paths_dict(
            dict_of_RAW_file_paths,
            mouse_ID=mouse_ID,
            clip_index=clip_index,
            source_base_path=base_path,
        )


class RunPacket:
    """
    This class is intended to hold all of the data from one mouse, mostly in the 
    form of a list of ClipPacket objects.

    The BigArrayRunPacket class defined below is generated from this and 
    contains roughly the same information, but takes more memory to store very 
    large numpy arrays of combined image data.
    """

    def __init__(self, clips, mouse_ID):
        self.clips = clips
        self.mouse_ID = mouse_ID

    @property
    def average_frame_rate(
        self,
    ):  # Actually, I decided that using the median would be a better idea
        median_diffs = np.array(
            [
                np.median(
                    np.diff(clip.frame_times[~np.isnan(clip.frame_times)])
                )
                for clip in self.clips
            ]
        )
        return np.median(1 / median_diffs[~np.isnan(median_diffs)])

    @classmethod
    def from_clip_paths(cls, clip_paths, mouse_ID):
        clips = []
        for clip_path in clip_paths:
            clip = ClipPacket.from_base_path(clip_path)
            if np.max(clip.frame_times) < 200:
                # There was at least one bmode file that failed this (seemingly 
                #    abnormal file format format)
                clips.append(clip)

        run_packet = cls(clips, mouse_ID)
        run_packet.offset_clips_by_file_name_timestamp()
        return run_packet

    def offset_clips_by_file_name_timestamp(self, nudge_apart=True):
        # This requires clips to have source_base_path fields ending in a time 
        #   stamp (e.g. _2022-07-02-11-44-24)

        def days_since_Y2K(year, month):
            days_in_each_month = [
                0,
                31,
                28,
                31,
                30,
                31,
                30,
                31,
                31,
                30,
                31,
                30,
                31,
            ]
            neglecting_leap_days = (int(year) - 2000) * 365 + np.cumsum(
                days_in_each_month
            )[int(month) - 1]
            leap_days = (int(year) - 2000) // 4 + (int(month) > 2) * (
                (int(year) - 2000) % 4 == 0
            )
            return neglecting_leap_days + leap_days

        seconds_since_Y2K = []
        for clip in self.clips:
            ts_elements = (
                (clip.source_base_path.split("_")[-1]).split(".")[0].split("-")
            )
            seconds_since_Y2K.append(
                int(ts_elements[-1])  # seconds
                + 60
                * (
                    int(ts_elements[-2])  # minutes
                    + 60
                    * (
                        int(ts_elements[-3])  # hours
                        + 24
                        * (
                            int(ts_elements[-4])  # days
                            + days_since_Y2K(ts_elements[-6], ts_elements[-5])
                        )
                    )
                )
            )

        seconds_since_run_start = (
            self.clips[np.argmin(seconds_since_Y2K)].physio_times[-1]
            + np.array(seconds_since_Y2K)
            - np.min(seconds_since_Y2K)
        )

        for i, clip in enumerate(self.clips):
            clip.time_offset = (
                seconds_since_run_start[i] - clip.physio_times[-1]
            )

        if nudge_apart:
            self.nudge_offsets()

    def nudge_offsets(self):
        abs_starts = np.array(
            [clip.physio_times[0] + clip.time_offset for clip in self.clips]
        )
        abs_ends = np.array(
            [clip.physio_times[-1] + clip.time_offset for clip in self.clips]
        )
        gap_lengths = abs_starts[1:] - abs_ends[:-1]

        adjacency_mask = (
            gap_lengths < 2.5
        )  # more than 2.5 seconds would mean a real gap in the data.
        collision_limits = -np.minimum(
            gap_lengths, 0
        )  # overlap due to rounding errors (time stamps only go to seconds)

        candidate_nudges = np.zeros(len(self.clips))
        candidate_nudges[1:-1] = np.diff(gap_lengths * adjacency_mask) / 3
        gap_changes = np.diff(candidate_nudges)
        new_gaps = np.maximum(
            (self.clips[0].physio_times[1] - self.clips[0].physio_times[0]),
            gap_lengths + gap_changes,
        )
        nudges = np.hstack([np.zeros(1), np.cumsum(new_gaps - gap_lengths)])
        nudges -= np.mean(nudges)

        new_offsets = abs_starts + nudges
        for new_offset, clip in zip(new_offsets, self.clips):
            clip.time_offset = new_offset

    def clip_coverage_plot(self, show=True, save_path=None):
        plt.figure(figsize=[12, 3], dpi=600)

        for i, clip in enumerate(self.clips):
            plt.plot(
                [
                    clip.physio_times[0] + clip.time_offset,
                    clip.physio_times[-1] + clip.time_offset,
                ],
                [i, i],
            )

        if not save_path == None:
            plt.savefig(
                save_path, bbox_inches="tight", transparent=True, dpi=600
            )

        if show:
            plt.show()


class BigArrayRunPacket:
    """
    BigArrayRunPacket objects are created from RunPacket objects and contain 
    most of the same data, but in the form of a few large numpy arrays, instead 
    of many small ones.
    """

    def __init__(
        self,
        bmode,
        contrast,
        breath,
        ekg_phase,
        meta_data,
        detect_injection=True,
        crop=True,
    ):

        self.bmode = bmode
        self.contrast = contrast
        self.breath = breath
        self.ekg_phase = ekg_phase
        self.meta_data = meta_data

        if detect_injection:
            self.offset_time_by_detected_influx()

        if crop:
            self.crop_excessive_data()

    def crop_excessive_data(self, threshold=0.9):
        valid_contrast_by_pixel = np.sum(~np.isnan(self.contrast), axis=0)
        good_pixels_mask = valid_contrast_by_pixel > threshold * np.max(
            valid_contrast_by_pixel
        )
        contrast_i_min = np.min(
            np.arange(self.contrast.shape[1])[np.any(good_pixels_mask, axis=1)]
        )
        contrast_i_max = np.max(
            np.arange(self.contrast.shape[1])[np.any(good_pixels_mask, axis=1)]
        )
        contrast_j_min = np.min(
            np.arange(self.contrast.shape[2])[np.any(good_pixels_mask, axis=0)]
        )
        contrast_j_max = np.max(
            np.arange(self.contrast.shape[2])[np.any(good_pixels_mask, axis=0)]
        )

        good_time_mask = self.meta_data["valid_contrast"] & (
            self.meta_data["frame_times"] > -10 * 60
        )
        contrast_t_min = np.min(
            np.arange(self.contrast.shape[0])[good_time_mask]
        )
        contrast_t_max = np.max(
            np.arange(self.contrast.shape[0])[good_time_mask]
        )

        self.meta_data["contrast_y_mm"] = self.meta_data["contrast_y_mm"][
            contrast_i_min:contrast_i_max
        ]
        self.meta_data["contrast_x_mm"] = self.meta_data["contrast_x_mm"][
            contrast_j_min:contrast_j_max
        ]
        self.contrast = self.contrast[
            contrast_t_min:contrast_t_max,
            contrast_i_min:contrast_i_max,
            contrast_j_min:contrast_j_max,
        ]

        self.bmode = self.bmode[contrast_t_min:contrast_t_max, :, :]
        self.breath = self.breath[contrast_t_min:contrast_t_max]
        self.ekg_phase = self.ekg_phase[contrast_t_min:contrast_t_max]
        self.meta_data["frame_times"] = self.meta_data["frame_times"][
            contrast_t_min:contrast_t_max
        ]
        self.meta_data["valid_bmode"] = self.meta_data["valid_bmode"][
            contrast_t_min:contrast_t_max
        ]
        self.meta_data["valid_contrast"] = self.meta_data["valid_contrast"][
            contrast_t_min:contrast_t_max
        ]
        self.meta_data["bmode_offness"] = self.meta_data["bmode_offness"][
            contrast_t_min:contrast_t_max
        ]

    def offset_time_by_detected_influx(self, show_plot=False):
        last_frame_of_clip = ~np.all(np.isnan(self.contrast), axis=(1, 2))
        last_frame_of_clip[:-1] &= ~last_frame_of_clip[1:]
        self.clip_numbers = (np.cumsum(np.roll(last_frame_of_clip, 1))).astype(
            np.int16
        )
        clip_medians = []
        for index in range(np.max(self.clip_numbers)):
            data_in_clip = self.contrast[self.clip_numbers == index, :, :]
            clip_medians.append(
                np.median(data_in_clip[~np.isnan(data_in_clip)])
            )

        last_baseline_index = np.argmax(
            np.diff(clip_medians / np.cumsum(clip_medians))
        )

        inj_time = np.min(
            self.meta_data["frame_times"][
                self.clip_numbers == last_baseline_index + 1
            ]
        )
        self.meta_data["frame_times"] = self.meta_data["frame_times"] - inj_time

        if show_plot:
            frame_medians = np.median(self.contrast, axis=(1, 2))
            plt.plot(self.meta_data["frame_times"], frame_medians)
            plt.plot(
                self.meta_data["frame_times"],
                self.clip_numbers - last_baseline_index + 1,
            )
            plt.plot(
                [0, 0], [0, np.max(frame_medians[~np.isnan(frame_medians)])]
            )
            plt.show()

    def block_dur_qunatiles(
        self, duration=1, block_shape=(17, 8), n_quantiles=20
    ):
        dur_number = np.floor(self.meta_data["frame_times"] / duration).astype(
            np.int32
        )
        start_number = np.min(dur_number)
        n_durations = np.max(dur_number) - start_number + 1

        block_array_dims = [
            self.contrast.shape[1] // block_shape[0],
            self.contrast.shape[2] // block_shape[1],
        ]
        left_margin = (
            self.contrast.shape[2] - block_shape[1] * block_array_dims[1]
        ) // 2

        quantile_values = np.arange(n_quantiles) / (n_quantiles - 1)

        # block_dur_quantiles[n_dur, i_block, j_block, k_quant] will be the 
        #   order of indices into the output array
        block_dur_quantiles = np.full(
            [
                n_durations,
                block_array_dims[0],
                block_array_dims[1],
                n_quantiles,
            ],
            np.nan,
            dtype=np.float32,
        )
        dur_times = np.full((n_durations), np.nan, dtype=np.float32)
        block_centers = np.full(
            [block_array_dims[0], block_array_dims[1], 2],
            np.nan,
            dtype=np.float32,
        )

        # First loop is just to record the locations of the centers of each 
        #   block.
        for i_block in range(block_array_dims[0]):
            for j_block in range(block_array_dims[1]):
                i_min_block = i_block * block_shape[0]
                i_max_block = (i_block + 1) * block_shape[0]
                j_min_block = j_block * block_shape[1] + left_margin
                j_max_block = (j_block + 1) * block_shape[1] + left_margin

                block_centers[i_block, j_block, 0] = np.mean(
                    self.meta_data["contrast_y_mm"][i_min_block:i_max_block]
                )
                block_centers[i_block, j_block, 1] = np.mean(
                    self.meta_data["contrast_x_mm"][j_min_block:j_max_block]
                )

        frame_has_image = ~np.all(np.isnan(self.contrast), axis=(1, 2))
        frame_indices = np.arange(len(self.meta_data["frame_times"]))

        for n_dur in range(n_durations):
            this_dur_mask = (
                dur_number == n_dur + start_number
            ) & frame_has_image

            print(f"processing {n_dur+1} of {n_durations}        ", end="\r")
            if np.any(this_dur_mask):
                dur_times[n_dur] = np.mean(
                    self.meta_data["frame_times"][this_dur_mask]
                )

                these_frame_indices = frame_indices[this_dur_mask]
                n_min_dur = np.min(these_frame_indices)
                n_max_dur = np.max(these_frame_indices)

                for i_block in range(block_array_dims[0]):
                    for j_block in range(block_array_dims[1]):
                        i_min_block = i_block * block_shape[0]
                        i_max_block = (i_block + 1) * block_shape[0]
                        j_min_block = j_block * block_shape[1] + left_margin
                        j_max_block = (j_block + 1) * block_shape[
                            1
                        ] + left_margin

                        pixels_in_block_dur = self.contrast[
                            n_min_dur:n_max_dur,
                            i_min_block:i_max_block,
                            j_min_block:j_max_block,
                        ]
                        if np.sum(
                            np.isnan(pixels_in_block_dur)
                        ) < 0.01 * np.prod(pixels_in_block_dur.shape):
                            block_dur_quantiles[n_dur, i_block, j_block, :] = (
                                np.quantile(
                                    pixels_in_block_dur[
                                        ~np.isnan(pixels_in_block_dur)
                                    ],
                                    quantile_values,
                                )
                            )

        return block_dur_quantiles, dur_times, block_centers, quantile_values

    def contrast_pixel_duration_stats(
        self,
        duration=1,  # an amount of time in seconds
        gated=True,
        pixel_stats=[
            "mean",
            "std",
            "median",
            "q25",
            "q75",
            "skew",
            "frac_high_clipped",
            "frac_low_clipped",
        ],
    ):
        dur_number = np.floor(self.meta_data["frame_times"] / duration).astype(
            np.int32
        )
        start_number = int(np.min(dur_number))
        n_durations = int(np.max(dur_number) - start_number + 1)

        stats_dict = {}
        for stat_key in pixel_stats:
            stats_dict[stat_key] = np.full(
                [n_durations, self.contrast.shape[1], self.contrast.shape[2]],
                np.nan,
                dtype=np.float32,
            )
        stats_dict["nonempty_frames"] = np.zeros(n_durations)
        stats_dict["duration_times"] = np.zeros(n_durations)

        frame_has_image = ~np.all(np.isnan(self.contrast), axis=(1, 2))

        for dur_index in range(n_durations):
            print(
                f"processing {dur_index+1} of {n_durations}        ", end="\r"
            )
            if gated == True:
                this_dur_mask = (
                    (dur_number == dur_index + start_number)
                    & frame_has_image
                    & (self.meta_data["bmode_offness"] < 0)
                )
            elif gated == False:
                this_dur_mask = (
                    dur_number == dur_index + start_number
                ) & frame_has_image
            if np.any(this_dur_mask):
                pixels_in_dur = self.contrast[this_dur_mask, :, :]
                stats_dict["nonempty_frames"][dur_index] = np.sum(
                    np.all(~np.isnan(pixels_in_dur), axis=(1, 2))
                )
                stats_dict["duration_times"][dur_index] = np.mean(
                    self.meta_data["frame_times"][this_dur_mask]
                )
                if stats_dict["nonempty_frames"][dur_index] > 3:
                    if "mean" in pixel_stats:
                        stats_dict["mean"][dur_index, :, :] = np.nanmean(
                            pixels_in_dur, axis=0
                        )
                    if "std" in pixel_stats:
                        stats_dict["std"][dur_index, :, :] = np.nanstd(
                            pixels_in_dur, axis=0
                        )
                    if "median" in pixel_stats:
                        stats_dict["median"][dur_index, :, :] = np.nanmedian(
                            pixels_in_dur, axis=0
                        )
                    if "frac_high_clipped" in pixel_stats:
                        stats_dict["frac_high_clipped"][dur_index, :, :] = (
                            np.sum(pixels_in_dur == 2**16 - 1)
                            / np.sum(~np.isnan(pixels_in_dur))
                        )
                    if "frac_low_clipped" in pixel_stats:
                        stats_dict["frac_high_clipped"][dur_index, :, :] = (
                            np.sum(pixels_in_dur == 0)
                            / np.sum(~np.isnan(pixels_in_dur))
                        )
                    if "q25" in pixel_stats:
                        stats_dict["q25"][dur_index, :, :] = np.nanquantile(
                            pixels_in_dur, 0.25, axis=0
                        )
                    if "q75" in pixel_stats:
                        stats_dict["q75"][dur_index, :, :] = np.nanquantile(
                            pixels_in_dur, 0.75, axis=0
                        )

        return stats_dict

    def contrast_block_frame_stats(
        self,
        block_shape=(17, 8),
        float_stats=["mean", "std", "median", "q25", "q75"],
        byte_stats=["n_zeros", "n_saturated"],
    ):
        block_array_shape = [
            self.contrast.shape[0],
            self.contrast.shape[1] // block_shape[0],
            self.contrast.shape[2] // block_shape[1],
        ]

        left_margin = (
            self.contrast.shape[2] - block_shape[1] * block_array_shape[2]
        ) // 2

        stats = float_stats + byte_stats
        block_frame_stats = dict(
            zip(
                stats,
                [np.zeros(block_array_shape, dtype=np.float32)]
                * len(float_stats)
                + [np.zeros(block_array_shape, dtype=np.uint8)]
                * len(byte_stats),
            )
        )

        for stat_key in block_frame_stats:
            block_frame_stats[stat_key] = copy.deepcopy(
                block_frame_stats[stat_key]
            )

        for i in range(block_array_shape[1]):
            print(f"processing {i+1} of {block_array_shape[1]}", end="\r")
            for j in range(block_array_shape[2]):
                i_min_block = i * block_shape[0]
                i_max_block = (i + 1) * block_shape[0]
                j_min_block = j * block_shape[1] + left_margin
                j_max_block = (j + 1) * block_shape[1] + left_margin

                if "mean" in stats:
                    block_frame_stats["mean"][:, i, j] = np.mean(
                        self.contrast[
                            :, i_min_block:i_max_block, j_min_block:j_max_block
                        ],
                        axis=(1, 2),
                    )
                if "std" in stats:
                    block_frame_stats["std"][:, i, j] = np.std(
                        self.contrast[
                            :, i_min_block:i_max_block, j_min_block:j_max_block
                        ],
                        axis=(1, 2),
                    )
                if "n_zeros" in stats:
                    block_frame_stats["n_zeros"][:, i, j] = np.sum(
                        self.contrast[
                            :, i_min_block:i_max_block, j_min_block:j_max_block
                        ]
                        == 0,
                        axis=(1, 2),
                    )
                if "n_saturated" in stats:
                    block_frame_stats["n_saturated"][:, i, j] = np.sum(
                        self.contrast[
                            :, i_min_block:i_max_block, j_min_block:j_max_block
                        ]
                        == 2**16 - 1,
                        axis=(1, 2),
                    )
                if "median" in stats:
                    block_frame_stats["median"][:, i, j] = np.median(
                        self.contrast[
                            :, i_min_block:i_max_block, j_min_block:j_max_block
                        ],
                        axis=(1, 2),
                    )
                if "q25" in stats:
                    block_frame_stats["q25"][:, i, j] = np.quantile(
                        self.contrast[
                            :, i_min_block:i_max_block, j_min_block:j_max_block
                        ],
                        0.25,
                        axis=(1, 2),
                    )
                if "q75" in stats:
                    block_frame_stats["q75"][:, i, j] = np.quantile(
                        self.contrast[
                            :, i_min_block:i_max_block, j_min_block:j_max_block
                        ],
                        0.75,
                        axis=(1, 2),
                    )

        return block_frame_stats

    @classmethod
    def from_clip_paths(
        cls, clip_paths, mouse_ID, detect_injection=True, crop=True
    ):
        run_packet = RunPacket.from_clip_paths(clip_paths, mouse_ID)
        return cls.from_run_packet(
            run_packet, detect_injection=detect_injection, crop=crop
        )

    @classmethod
    def from_run_packet(cls, run_packet, detect_injection=True, crop=True):
        frame_rate = run_packet.average_frame_rate

        # Arrays will cover all time with physio data, which shoould be a 
        #   superset of time with image data.
        start_time_abs = (
            np.min(
                run_packet.clips[0].physio_times[
                    ~np.isnan(run_packet.clips[0].physio_times)
                ]
            )
            + run_packet.clips[0].time_offset
        )
        end_time_abs = (
            np.max(
                run_packet.clips[-1].physio_times[
                    ~np.isnan(run_packet.clips[-1].physio_times)
                ]
            )
            + run_packet.clips[-1].time_offset
        )
        n_frames = int(
            np.ceil((end_time_abs - start_time_abs) * frame_rate) + 1
        )
        frame_times = np.arange(n_frames).astype(np.float64) / frame_rate
        valid_bmode = np.zeros(frame_times.shape, dtype=bool)
        valid_contrast = np.zeros(frame_times.shape, dtype=bool)

        # At least for now, the only physio data perserved will be the breath 
        #   signal down-sampled to image times.
        downsampled_breath_array = np.full(n_frames, np.nan, dtype=np.float32)
        ekg_phase_array = np.full(n_frames, np.nan, dtype=np.float32)

        for i, clip in enumerate(run_packet.clips):
            start_time = clip.time_offset + clip.physio_times[0]
            first_frame_index = int(
                np.maximum(0, np.ceil(start_time * frame_rate))
            )
            end_time = clip.time_offset + clip.physio_times[-1]
            last_frame_index = int(np.floor(end_time * frame_rate))
            if last_frame_index > first_frame_index:

                resamp_times = (
                    np.arange(last_frame_index + 1 - first_frame_index)
                    / frame_rate
                    + frame_times[first_frame_index]
                )
                physio_sample_rate = 8000
                resamp_indices = np.round(
                    (resamp_times - clip.time_offset) * physio_sample_rate
                ).astype(int)

                downsampled_breath_array[
                    first_frame_index : first_frame_index + len(resamp_indices)
                ] = clip.breath[resamp_indices]

                ekg_phase_array[
                    first_frame_index : first_frame_index + len(resamp_indices)
                ] = clip.ekg_phase[resamp_indices]

        # bmode is reltively easy, since every frame can be assumed to have the 
        #   same shape.
        bmode_big_array = np.full(
            [
                n_frames,
                run_packet.clips[0].bmode_pixels.shape[1],
                run_packet.clips[0].bmode_pixels.shape[2],
            ],
            np.nan,
            dtype=np.uint8,
        )
        bmode_offness_signal = np.full(n_frames, np.nan, dtype=np.float32)

        print(" ")
        for clip in run_packet.clips:
            start_time = clip.time_offset + clip.frame_times[0]
            start_index = np.argmin(np.abs(frame_times - start_time))

            bmode_big_array[
                start_index : start_index + clip.bmode_pixels.shape[0], :, :
            ] = clip.bmode_pixels
            bmode_offness_signal[
                start_index : start_index + clip.bmode_pixels.shape[0]
            ] = clip.bmode_offness_signal
            valid_bmode[
                start_index : start_index + clip.bmode_pixels.shape[0]
            ] = True

        # The following contrast-to-bmode mapping means that each contrast pixel
        #   will correspond to one side of a bmode pixel (as opposed to half of 
        #   them being centered on a bmode pixel and half centered between two).
        bmode_pixel_width = run_packet.clips[0].meta_data["bmode_pixel_width"]
        bmode_x_mm = run_packet.clips[0].meta_data["bmode_x_mm"]
        bmode_y_mm = run_packet.clips[0].meta_data["bmode_y_mm"]

        contrast_candidate_x_mm = np.zeros(2 * (len(bmode_x_mm) - 1))
        contrast_candidate_x_mm[::2] = bmode_x_mm[:-1] + bmode_pixel_width * (
            1 / 4
        )
        contrast_candidate_x_mm[1::2] = bmode_x_mm[:-1] + bmode_pixel_width * (
            3 / 4
        )

        contrast_min_x_index = np.min(
            [
                np.argmin(
                    np.abs(
                        clip.meta_data["contrast_extent"][0]
                        - contrast_candidate_x_mm
                    )
                )
                for clip in run_packet.clips
            ]
        )
        contrast_max_x_index = np.max(
            [
                np.argmin(
                    np.abs(
                        clip.meta_data["contrast_extent"][1]
                        - contrast_candidate_x_mm
                    )
                )
                for clip in run_packet.clips
            ]
        )
        contrast_min_y_index = np.min(
            [
                np.argmin(
                    np.abs(clip.meta_data["contrast_extent"][3] - bmode_y_mm)
                )
                for clip in run_packet.clips
            ]
        )
        contrast_max_y_index = np.max(
            [
                np.argmin(
                    np.abs(clip.meta_data["contrast_extent"][2] - bmode_y_mm)
                )
                for clip in run_packet.clips
            ]
        )

        contrast_x_mm = contrast_candidate_x_mm[
            contrast_min_x_index : contrast_max_x_index + 1
        ]
        contrast_y_mm = bmode_y_mm[
            contrast_min_y_index : contrast_max_y_index + 1
        ]

        contrast_big_array = np.full(
            [n_frames, len(contrast_y_mm), len(contrast_x_mm)],
            np.nan,
            dtype=np.float32,
        )
        for i, clip in enumerate(run_packet.clips):

            start_time = clip.time_offset + clip.frame_times[0]
            start_index = int(np.argmin(np.abs(frame_times - start_time)))

            this_extent = clip.meta_data["contrast_extent"]

            this_clip_top_y_index = int(
                np.argmin(np.abs(contrast_y_mm[0] - this_extent[2]))
            )
            this_clip_left_x_index = int(
                np.argmin(np.abs(contrast_x_mm[0] - this_extent[0]))
            )

            contrast_big_array[
                start_index : start_index + clip.contrast_pixels.shape[0],
                this_clip_top_y_index : this_clip_top_y_index
                + clip.contrast_pixels.shape[1],
                this_clip_left_x_index : this_clip_left_x_index
                + clip.contrast_pixels.shape[2],
            ] = clip.contrast_pixels
            valid_contrast[
                start_index : start_index + clip.contrast_pixels.shape[0]
            ] = True

        meta_data = {
            "frame_rate": frame_rate,
            "start_time_abs": start_time_abs,
            "end_time_abs": end_time_abs,
            "contrast_x_mm": contrast_x_mm,
            "contrast_y_mm": contrast_y_mm,
            "bmode_x_mm": bmode_x_mm,
            "bmode_y_mm": bmode_y_mm,
            "frame_times": frame_times,
            "valid_bmode": valid_bmode,
            "bmode_offness": bmode_offness_signal,
            "valid_contrast": valid_contrast,
        }

        return cls(
            bmode_big_array,
            contrast_big_array,
            downsampled_breath_array,
            ekg_phase_array,
            meta_data,
            detect_injection=detect_injection,
            crop=crop,
        )

    def write_to_npy_files(self, base_folder, base_name):
        if not os.path.exists(base_folder):
            os.makedirs(base_folder)

        for item in self.meta_data:
            file_name = base_name + item + ".npy"
            np.save(
                os.path.join(base_folder, file_name),
                np.array(self.meta_data[item]),
            )

        np.save(
            os.path.join(base_folder, base_name + "contrast.npy"), self.contrast
        )
        np.save(os.path.join(base_folder, base_name + "bmode.npy"), self.bmode)
        np.save(
            os.path.join(base_folder, base_name + "breath.npy"), self.breath
        )
        np.save(
            os.path.join(base_folder, base_name + "ekg_phase.npy"),
            self.ekg_phase,
        )

    def make_thumbnails(
        self, base_folder=None, base_name=None, show_images=False
    ):
        contrast_thumbnail = np.nanmedian(self.contrast, axis=0)
        bmode_thumbnail = np.nanmedian(self.bmode, axis=0)

        if not base_folder == None:
            if not os.path.exists(base_folder):
                os.makedirs(base_folder)
            if base_name == None:
                base_name = ""
            np.save(
                os.path.join(
                    base_folder, base_name + "contrast_median_image.npy"
                ),
                contrast_thumbnail,
            )
            np.save(
                os.path.join(base_folder, base_name + "bmode_median_image.npy"),
                bmode_thumbnail,
            )

        if show_images:
            fig, (ax1, ax2) = plt.subplots(2, 1)
            ax1.imshow(
                np.log(contrast_thumbnail + 1), cmap="gray", aspect=2 / 4.24
            )
            ax2.imshow(bmode_thumbnail, cmap="gray", aspect=1 / 4.24)
            fig.show()

    @classmethod
    def from_npy_files(
        cls, base_folder, base_name, detect_injection=False, crop=False
    ):
        meta_data = {
            "frame_rate": unwrap_if_tuple(
                np.load(os.path.join(base_folder, base_name + "frame_rate.npy"))
            ),
            "start_time_abs": unwrap_if_tuple(
                np.load(
                    os.path.join(base_folder, base_name + "start_time_abs.npy")
                )
            ),
            "end_time_abs": unwrap_if_tuple(
                np.load(
                    os.path.join(base_folder, base_name + "end_time_abs.npy")
                )
            ),
            "contrast_x_mm": unwrap_if_tuple(
                np.load(
                    os.path.join(base_folder, base_name + "contrast_x_mm.npy")
                )
            ),
            "contrast_y_mm": unwrap_if_tuple(
                np.load(
                    os.path.join(base_folder, base_name + "contrast_y_mm.npy")
                )
            ),
            "bmode_x_mm": unwrap_if_tuple(
                np.load(os.path.join(base_folder, base_name + "bmode_x_mm.npy"))
            ),
            "bmode_y_mm": unwrap_if_tuple(
                np.load(os.path.join(base_folder, base_name + "bmode_y_mm.npy"))
            ),
            "frame_times": unwrap_if_tuple(
                np.load(
                    os.path.join(base_folder, base_name + "frame_times.npy")
                )
            ),
            "valid_bmode": unwrap_if_tuple(
                np.load(
                    os.path.join(base_folder, base_name + "valid_bmode.npy")
                )
            ),
            "valid_contrast": unwrap_if_tuple(
                np.load(
                    os.path.join(base_folder, base_name + "valid_contrast.npy")
                )
            ),
            "bmode_offness": unwrap_if_tuple(
                np.load(
                    os.path.join(base_folder, base_name + "bmode_offness.npy")
                )
            ),
        }

        return cls(
            unwrap_if_tuple(
                np.load(os.path.join(base_folder, base_name + "bmode.npy"))
            ),
            unwrap_if_tuple(
                np.load(os.path.join(base_folder, base_name + "contrast.npy"))
            ),
            unwrap_if_tuple(
                np.load(os.path.join(base_folder, base_name + "breath.npy"))
            ),
            unwrap_if_tuple(
                np.load(os.path.join(base_folder, base_name + "ekg_phase.npy"))
            ),
            meta_data,
            detect_injection=detect_injection,
            crop=crop,
        )


# Functions to help with reading data from files
def unwrap_if_tuple(possible_tuple):
    if type(possible_tuple) is tuple:
        return possible_tuple[0]
    else:
        return possible_tuple


def identify_runs(raw_folder_paths):
    clip_paths_dict = {}
    for raw_folder_path in raw_folder_paths:
        for file in os.listdir(raw_folder_path):
            if len(file.split(".")) > 2:
                if file.split(".")[-2] in ["raw", "RAW"]:
                    clip_path = os.path.join(
                        raw_folder_path, "".join(file.split(".")[:-2])
                    )
                    if clip_path in clip_paths_dict:
                        clip_paths_dict[clip_path].append(
                            clip_path + "." + ".".join(file.split(".")[-2:])
                        )
                    else:
                        clip_paths_dict[clip_path] = [
                            clip_path + "." + ".".join(file.split(".")[-2:])
                        ]
    run_paths_dict = {}
    for clip_path in clip_paths_dict:
        for file_path in clip_paths_dict[clip_path]:
            if file_path.split(".")[-1] == "xml":
                series_name = read_raw_xml(file_path)["Series-Name"]
                run_key = series_name + " in " + os.path.dirname(clip_path)
                if run_key in run_paths_dict:
                    run_paths_dict[run_key].append(clip_path)
                else:
                    run_paths_dict[run_key] = [clip_path]
                break

    return run_paths_dict


def read_raw_xml(xml_path):
    """
    Reads .raw.xml files exported from VEVO Lab and returns a python dictionary
    of parameters from it.
    """

    assert xml_path[-8:] == ".raw.xml"

    param_dict = {}

    with open(xml_path, "r") as xml_file:
        for line in xml_file:
            if "<parameter name=" in line:
                _, key, _, value, *tail = line.split('"')

                if not "Name" in key:
                    try:
                        value = int(value)
                    except:
                        try:
                            value = float(value)
                        except:
                            value = value

                param_dict[key] = value

                if tail[0] == " units=":
                    param_dict[key + "_units"] = tail[1]

    return param_dict


def read_raw_pixels(
    raw_pixels_path,
    n_frames=None,
    n_rows=848,
    n_cols=256,
    file_header_bytes=40,
    frame_header_bytes=56,
    data_type="uint8",
    return_headers=False,
):
    if data_type in ["uint8", np.uint8]:
        bytes_per_pixel = 1
    elif data_type in ["uint16", np.uint16]:
        bytes_per_pixel = 2
    elif data_type in ["uint32", np.uint32]:
        bytes_per_pixel = 4
    else:
        warnings.warn(
            f"read_raw_pixels does not recognize data_type {data_type}"
        )


    file_size = os.stat(raw_pixels_path).st_size
    bytes_per_frame = (
        bytes_per_pixel * n_rows * n_cols + frame_header_bytes
    )  # + col_header_bytes*n_cols
    offset_to_first = file_header_bytes + frame_header_bytes

    if n_frames == None:  # if n_frames is not provided, it can be calculated
        n_frames = int(
            (file_size - file_header_bytes) // bytes_per_frame
        )  # alternative method

    image_data = np.zeros([n_frames, n_rows, n_cols], dtype=data_type)
    for i in range(n_frames):
        try:
            image_data[i, :, :] = (
                np.fromfile(
                    raw_pixels_path,
                    dtype=data_type,
                    count=n_rows * n_cols,
                    offset=offset_to_first + i * bytes_per_frame,
                )
                .reshape([n_cols, n_rows])
                .transpose()
            )
        except:
            print(f"Image reconstruction failed at frame {i+1} of {n_frames}.")

    if return_headers:
        file_header = np.fromfile(
            raw_pixels_path, dtype=np.uint8, count=file_header_bytes
        )

        frame_headers = np.zeros(
            [n_frames, frame_header_bytes // 4], dtype=np.uint32
        )
        for i in range(n_frames):
            try:
                frame_headers[i, :] = np.fromfile(
                    raw_pixels_path,
                    dtype=np.uint32,
                    count=frame_header_bytes // 4,
                    offset=file_header_bytes + i * bytes_per_frame,
                )
            except:
                print(f"Frame header read failed at frame {i+1} of {n_frames}.")

        return image_data, frame_headers, file_header

    else:
        return image_data


def read_bmode_pixels(bmode_path, return_headers=False):
    return read_raw_pixels(
        bmode_path,
        data_type="uint8",
        n_rows=848,
        n_cols=256,
        file_header_bytes=40,
        frame_header_bytes=56,
        return_headers=return_headers,
    )


def read_contrast_pixels(contrast_path, frame_shape, return_headers=False):
    return read_raw_pixels(
        contrast_path,
        data_type="uint16",
        n_rows=frame_shape[0],
        n_cols=frame_shape[1],
        file_header_bytes=40,
        frame_header_bytes=72,
        return_headers=return_headers,
    )


def read_physio_raw(physio_path):
    file_header_length = 24  # unit: 2 bytes for int16
    frame_header_length = 28  # unit: 2 bytes for int16
    data_seg_length = 256  # unit: 2 bytes for int16
    frame_length = frame_header_length + 4 * data_seg_length

    full_file = np.fromfile(
        physio_path,
        dtype=np.int16,
        offset=file_header_length * 2,  # times 2 for bytes
    )

    n_frames = len(full_file) // frame_length

    excess_length = len(full_file) - frame_length * n_frames
    if not excess_length == 0:
        # All my files seem to have excess_length of 2096 bytes. Probably some 
        #   kind of "footer" data.
        full_file = full_file[:-excess_length]

    full_file.shape = (n_frames, frame_length)

    frame_headers = full_file[:, :file_header_length]
    ekg = full_file[
        :, file_header_length : file_header_length + data_seg_length
    ]
    breath = full_file[
        :,
        file_header_length
        + data_seg_length : file_header_length
        + data_seg_length * 2,
    ]

    file_header = np.fromfile(
        physio_path, dtype=np.uint8, count=file_header_length
    )

    return breath.flatten(), ekg.flatten(), frame_headers, file_header


# tests
def run_identification_test():
    run_paths_dict = identify_runs(
        list(pd.read_excel(r"./VEVO_RAW_file_locations.xlsx")["RAW_paths"])
    )

    for run_key in run_paths_dict:
        print(f"{len(run_paths_dict[run_key])} clips in {run_key}")

    pd.DataFrame({"RunKeys": list(run_paths_dict.keys())}).to_excel(
        "./RunKeys.xlsx"
    )


def run_packet_read_test():
    run_paths_dict = identify_runs(
        list(pd.read_excel(r"./VEVO_RAW_file_locations.xlsx")["RAW_paths"])
    )

    run_info_df = pd.read_excel(r"./RunKeys Info.xlsx")

    test_run_row_index = 10  # just selecting a run somewhat randomly

    run_key = run_info_df["RunKeys"][test_run_row_index]
    mouse_ID = run_info_df["extended_mouse_ID"][test_run_row_index]

    run_packet = RunPacket.from_clip_paths(run_paths_dict[run_key], mouse_ID)
    run_packet.clip_coverage_plot()


def big_array_run_packet_read_test():
    run_paths_dict = identify_runs(
        list(pd.read_excel(r"./VEVO_RAW_file_locations.xlsx")["RAW_paths"])
    )

    run_info_df = pd.read_excel(r"./RunKeys Info.xlsx")

    test_run_row_index = 10  # just selecting a run somewhat randomly

    run_key = run_info_df["RunKeys"][test_run_row_index]
    mouse_ID = run_info_df["extended_mouse_ID"][test_run_row_index]

    big_array_run_packet = BigArrayRunPacket.from_clip_paths(
        run_paths_dict[run_key], mouse_ID
    )


if __name__ == "__main__":
    # run_identification_test()
    # run_packet_read_test()
    big_array_run_packet_read_test()
